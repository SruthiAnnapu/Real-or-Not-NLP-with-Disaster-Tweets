{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop(labels = 'target', axis = 1)\n",
    "train_y = train.drop(labels = ['id','keyword','location','text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimention of train data :  (7613, 4)\n",
      "Dimention of train label :  (7613, 1)\n",
      "Dimention of test data :  (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimention of train data : \",train_X.shape)\n",
    "print(\"Dimention of train label : \",train_y.shape)\n",
    "print(\"Dimention of test data : \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null Cells in --- id ----are  0\n",
      "Number of Null Cells in --- keyword ----are  61\n",
      "Number of Null Cells in --- location ----are  2533\n",
      "Number of Null Cells in --- text ----are  0\n"
     ]
    }
   ],
   "source": [
    "#Null Data Field\n",
    "for c in train_X.columns:   \n",
    "    print(\"Number of Null Cells in ---\",c,\"----are \",train_X[c].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_data = pd.concat([train_X, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id keyword location  \\\n",
       "0          1     NaN      NaN   \n",
       "1          4     NaN      NaN   \n",
       "2          5     NaN      NaN   \n",
       "3          6     NaN      NaN   \n",
       "4          7     NaN      NaN   \n",
       "...      ...     ...      ...   \n",
       "10871  10861     NaN      NaN   \n",
       "10872  10865     NaN      NaN   \n",
       "10873  10868     NaN      NaN   \n",
       "10874  10874     NaN      NaN   \n",
       "10875  10875     NaN      NaN   \n",
       "\n",
       "                                                    text  \n",
       "0      Our Deeds are the Reason of this #earthquake M...  \n",
       "1                 Forest fire near La Ronge Sask. Canada  \n",
       "2      All residents asked to 'shelter in place' are ...  \n",
       "3      13,000 people receive #wildfires evacuation or...  \n",
       "4      Just got sent this photo from Ruby #Alaska as ...  \n",
       "...                                                  ...  \n",
       "10871  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "10872  Storm in RI worse than last hurricane. My city...  \n",
       "10873  Green Line derailment in Chicago http://t.co/U...  \n",
       "10874  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "10875  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[10876 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d31928c2d799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags(data):\n",
    "    pattern = re.compile(r'[@|#][a-zA-Z]+')\n",
    "    matches = pattern.findall(data)\n",
    "    tags = [match[1:].lower() for match in matches]\n",
    "    return ' '.join(tags)\n",
    "\n",
    "def striplinks(data):\n",
    "    cleanr = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    clean = re.sub(cleanr, ' ', data)\n",
    "    return clean\n",
    "    \n",
    "def alphanumeric(data):\n",
    "    cleanr = re.compile(r'[^A-Za-z]+')\n",
    "    clean = re.sub(cleanr, ' ', data)\n",
    "    return clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer  = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10876/10876 [00:05<00:00, 1871.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run this subsection : 0:00:05.826058\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "with open('processed_total_data.csv', 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['tweets'])\n",
    "    for i in tqdm(range(10876)):\n",
    "        \n",
    "        #raw data\n",
    "        keyword = str(Total_data['keyword'][i])\n",
    "        location = str(Total_data['location'][i])\n",
    "        text = str(Total_data['text'][i])\n",
    "   \n",
    "        #extract tags \n",
    "        tags_str = stemmer.stem(tags(text))\n",
    "        #print(tags_str)\n",
    "        #strip links and alphanumberic charcters\n",
    "        text_sl = striplinks(text)\n",
    "        text_sl_an = alphanumeric(text_sl)\n",
    "        #print(text_sl)\n",
    "        #print(text_sl_an)\n",
    "        #removing stopwords and stem the text       \n",
    "        text_words = word_tokenize(text_sl_an.lower())\n",
    "        text_sl_an_sw_st = ' '.join(stemmer.stem(j) for j in text_words if j not in stop_words )\n",
    "        #print(text_words)\n",
    "        #print(text_sl_an_sw_st)\n",
    "        #assigning weights\n",
    "        processed_list = []\n",
    "        if(tags_str != ''):\n",
    "            processed_list.extend([tags_str,tags_str,tags_str,text_sl_an_sw_st])\n",
    "        else:\n",
    "            processed_list.extend([text_sl_an_sw_st])\n",
    "        if (keyword != 'nan'):\n",
    "            #print(keyword)\n",
    "            processed_list.extend([keyword,keyword])\n",
    "        if (location != 'nan'):\n",
    "            location_words = word_tokenize(alphanumeric(location))\n",
    "            location_str = \" \".join(location_words)\n",
    "            processed_list.extend([location_str])\n",
    "        \n",
    "        #saving string\n",
    "        \n",
    "        processed_string = \" \".join(processed_list)\n",
    "\n",
    "        writer.writerow([processed_string])\n",
    "print(\"Time to run this subsection :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_data['keyword'][0] == float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(Total_data['keyword'][0]) == 'nans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'painting is funny thing swim'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"weren't\", 'such', \"she's\", 'these', 'if', 'those', 'and', 'here', 'should', 'its', 'itself', 'i', 'll', 'ours', 'then', 'ma', 'further', 'to', \"haven't\", 't', 'each', 'hadn', 'my', 'while', 'she', 'into', 'him', \"you're\", 'me', 'so', \"hadn't\", \"isn't\", 'd', 'out', 'myself', 'do', 'himself', \"aren't\", 'ourselves', 'when', 'why', 'in', 'too', 'doing', 'until', \"you'd\", \"shouldn't\", 'isn', 'than', 're', 'her', 'not', \"couldn't\", 'are', \"it's\", 'be', 'it', 'just', \"needn't\", 'yourselves', 'had', 'below', 'their', 'up', 'your', \"that'll\", 'aren', 'was', 'm', 'only', 'or', 'that', 'did', 'a', \"wasn't\", \"you've\", 'by', 'between', 'more', 'we', 'being', 'same', 'herself', 'am', 'down', 'don', 'y', 'wouldn', 'no', 's', 'against', 'very', 'any', 'theirs', 'how', 'ain', 'through', 'hasn', \"mightn't\", 'haven', 'all', 'which', 'other', \"you'll\", 'at', 'them', \"hasn't\", 'an', 'does', 'now', 'his', 'some', 'themselves', 'whom', 'once', 'shouldn', 'will', 'most', 'couldn', 'been', 'the', 'but', 'own', 'what', 'nor', 'over', 'above', 'of', \"don't\", 'mightn', 'won', 'both', 'yourself', 'having', 'about', 'our', 'before', 'o', 'shan', \"didn't\", 'has', 'didn', 'on', 'during', 'for', 'from', 'he', 'again', 'were', 'doesn', \"mustn't\", 'needn', 'as', \"won't\", 'can', 'there', 'yours', 've', 'hers', 'with', 'because', \"should've\", 'is', 'weren', 'off', \"doesn't\", 'you', 'have', 'this', \"wouldn't\", 'they', 'after', 'under', 'wasn', 'mustn', \"shan't\", 'who', 'where', 'few'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizing data with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_total_data = pd.read_csv(\"processed_total_data.csv\")\n",
    "processed_total_data.replace(np.nan, '', regex=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run this Subsection  0:00:01.399283\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "#smooth_idf=True by default so smoothing is done by defult.\n",
    "#norm is l2 by default.\n",
    "#subliner is used False by default.\n",
    "vectorizer = TfidfVectorizer(min_df = 0.0001, \n",
    "                             max_features = 100000, \n",
    "                             tokenizer = lambda x: x.split(),\n",
    "                             ngram_range = (1,4))\n",
    "total_tfidf = vectorizer.fit_transform( processed_total_data['tweets'])\n",
    "\n",
    "print(\"Time to run this Subsection \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 35879)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = total_tfidf[0:7613]\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tfidf, train_y, \n",
    "                                                    test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: C=1;penalty='l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy 0.7918581746552856\n",
      "Macro precision_recall_fscore_support\n",
      "(0.7907704684243244, 0.7803714921389358, 0.7838016598502615, None)\n",
      "Micro precision_recall_fscore_support\n",
      "(0.7918581746552856, 0.7918581746552856, 0.7918581746552856, None)\n",
      "Weighted precision_recall_fscore_support\n",
      "(0.7914446665013665, 0.7918581746552856, 0.7899673599561473, None)\n"
     ]
    }
   ],
   "source": [
    "Model1 = LogisticRegression(C=1,penalty='l2')\n",
    "Model1.fit(X_train, y_train)\n",
    "y_pred = Model1.predict(X_test)\n",
    "\n",
    "print(\"Acuracy\", accuracy_score(y_test['target'], y_pred))\n",
    "print(\"Macro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='macro'))\n",
    "print(\"Micro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='micro'))\n",
    "print(\"Weighted precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2.1: Gaussian Naive Bayes\n",
    "Model 2.2: Bernoulli Naive Bayes\n",
    "Model 2.3: Complement Naive Bayes\n",
    "Model 2.4: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy 0.7255416940249507\n",
      "Macro precision_recall_fscore_support\n",
      "(0.7203331995182658, 0.712872823178063, 0.7151421483698219, None)\n",
      "Micro precision_recall_fscore_support\n",
      "(0.7255416940249507, 0.7255416940249507, 0.7255416940249507, None)\n",
      "Weighted precision_recall_fscore_support\n",
      "(0.7236158641233194, 0.7255416940249507, 0.7231830341856432, None)\n"
     ]
    }
   ],
   "source": [
    "Model21 = GaussianNB()\n",
    "Model21.fit(X_train.toarray(), y_train)\n",
    "y_pred = Model21.predict(X_test.toarray())\n",
    "\n",
    "print(\"Acuracy\", accuracy_score(y_test['target'], y_pred))\n",
    "print(\"Macro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='macro'))\n",
    "print(\"Micro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='micro'))\n",
    "print(\"Weighted precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy 0.7734734077478661\n",
      "Macro precision_recall_fscore_support\n",
      "(0.8148599786170851, 0.741941483641441, 0.7470787714835148, None)\n",
      "Micro precision_recall_fscore_support\n",
      "(0.7734734077478661, 0.7734734077478661, 0.7734734077478661, None)\n",
      "Weighted precision_recall_fscore_support\n",
      "(0.8025912323119806, 0.7734734077478661, 0.759149489287334, None)\n"
     ]
    }
   ],
   "source": [
    "Model22 = BernoulliNB()\n",
    "Model22.fit(X_train.toarray(), y_train)\n",
    "y_pred = Model22.predict(X_test.toarray())\n",
    "\n",
    "print(\"Acuracy\", accuracy_score(y_test['target'], y_pred))\n",
    "print(\"Macro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='macro'))\n",
    "print(\"Micro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='micro'))\n",
    "print(\"Weighted precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy 0.788575180564675\n",
      "Macro precision_recall_fscore_support\n",
      "(0.7873781505225257, 0.7769160793052505, 0.780333652257734, None)\n",
      "Micro precision_recall_fscore_support\n",
      "(0.788575180564675, 0.788575180564675, 0.788575180564675, None)\n",
      "Weighted precision_recall_fscore_support\n",
      "(0.7881160457539876, 0.788575180564675, 0.7866195636782821, None)\n"
     ]
    }
   ],
   "source": [
    "Model23 = ComplementNB()\n",
    "Model23.fit(X_train.toarray(), y_train)\n",
    "y_pred = Model23.predict(X_test.toarray())\n",
    "\n",
    "print(\"Acuracy\", accuracy_score(y_test['target'], y_pred))\n",
    "print(\"Macro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='macro'))\n",
    "print(\"Micro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='micro'))\n",
    "print(\"Weighted precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy 0.7898883782009193\n",
      "Macro precision_recall_fscore_support\n",
      "(0.8004832111577849, 0.7701268982733513, 0.7761734178377881, None)\n",
      "Micro precision_recall_fscore_support\n",
      "(0.7898883782009193, 0.7898883782009193, 0.7898883782009194, None)\n",
      "Weighted precision_recall_fscore_support\n",
      "(0.7959769022441842, 0.7898883782009193, 0.7843587390094182, None)\n"
     ]
    }
   ],
   "source": [
    "Model24= MultinomialNB()\n",
    "Model24.fit(X_train.toarray(), y_train)\n",
    "y_pred = Model24.predict(X_test.toarray())\n",
    "\n",
    "print(\"Acuracy\", accuracy_score(y_test['target'], y_pred))\n",
    "print(\"Macro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='macro'))\n",
    "print(\"Micro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='micro'))\n",
    "print(\"Weighted precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy 0.5738673670387393\n",
      "Macro precision_recall_fscore_support\n",
      "(0.28693368351936965, 0.5, 0.3646224447225699, None)\n",
      "Micro precision_recall_fscore_support\n",
      "(0.5738673670387393, 0.5738673670387393, 0.5738673670387393, None)\n",
      "Weighted precision_recall_fscore_support\n",
      "(0.32932375495197513, 0.5738673670387393, 0.4184898446323389, None)\n"
     ]
    }
   ],
   "source": [
    "Model31 = svm.SVC(kernel='rbf') #rbf by default svm.SVC()\n",
    "Model31.fit(X_train, y_train)\n",
    "y_pred = Model31.predict(X_test)\n",
    "\n",
    "print(\"Acuracy\", accuracy_score(y_test['target'], y_pred))\n",
    "print(\"Macro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='macro'))\n",
    "print(\"Micro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='micro'))\n",
    "print(\"Weighted precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy 0.7826657912015759\n",
      "Macro precision_recall_fscore_support\n",
      "(0.7802441549948226, 0.7719656715312768, 0.7748362680688006, None)\n",
      "Micro precision_recall_fscore_support\n",
      "(0.7826657912015759, 0.7826657912015759, 0.7826657912015759, None)\n",
      "Weighted precision_recall_fscore_support\n",
      "(0.7818326918651418, 0.7826657912015759, 0.7810392353394852, None)\n"
     ]
    }
   ],
   "source": [
    "Model32 = svm.SVC(kernel='linear')\n",
    "Model32.fit(X_train, y_train)\n",
    "y_pred = Model32.predict(X_test)\n",
    "\n",
    "print(\"Acuracy\", accuracy_score(y_test['target'], y_pred))\n",
    "print(\"Macro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='macro'))\n",
    "print(\"Micro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='micro'))\n",
    "print(\"Weighted precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy 0.7518056467498359\n",
      "Macro precision_recall_fscore_support\n",
      "(0.7613299103996778, 0.7288144408048995, 0.7333179539061891, None)\n",
      "Micro precision_recall_fscore_support\n",
      "(0.7518056467498359, 0.7518056467498359, 0.7518056467498359, None)\n",
      "Weighted precision_recall_fscore_support\n",
      "(0.7576159427719232, 0.7518056467498359, 0.74369134764639, None)\n"
     ]
    }
   ],
   "source": [
    "Model41 = RandomForestClassifier(random_state=0)\n",
    "Model41.fit(X_train, y_train)\n",
    "y_pred = Model41.predict(X_test)\n",
    "\n",
    "print(\"Acuracy\", accuracy_score(y_test['target'], y_pred))\n",
    "print(\"Macro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='macro'))\n",
    "print(\"Micro precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='micro'))\n",
    "print(\"Weighted precision_recall_fscore_support\")\n",
    "print(precision_recall_fscore_support(y_test['target'], y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
