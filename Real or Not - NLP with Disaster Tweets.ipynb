{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop(labels = 'target', axis = 1)\n",
    "train_y = train.drop(labels = ['id','keyword','location','text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimention of train data :  (7613, 4)\n",
      "Dimention of train label :  (7613, 1)\n",
      "Dimention of test data :  (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimention of train data : \",train_X.shape)\n",
    "print(\"Dimention of train label : \",train_y.shape)\n",
    "print(\"Dimention of test data : \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null Cells in --- id ----are  0\n",
      "Number of Null Cells in --- keyword ----are  61\n",
      "Number of Null Cells in --- location ----are  2533\n",
      "Number of Null Cells in --- text ----are  0\n"
     ]
    }
   ],
   "source": [
    "#Null Data Field\n",
    "for c in train_X.columns:   \n",
    "    print(\"Number of Null Cells in ---\",c,\"----are \",train_X[c].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_data = pd.concat([train_X, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id keyword location  \\\n",
       "0          1     NaN      NaN   \n",
       "1          4     NaN      NaN   \n",
       "2          5     NaN      NaN   \n",
       "3          6     NaN      NaN   \n",
       "4          7     NaN      NaN   \n",
       "...      ...     ...      ...   \n",
       "10871  10861     NaN      NaN   \n",
       "10872  10865     NaN      NaN   \n",
       "10873  10868     NaN      NaN   \n",
       "10874  10874     NaN      NaN   \n",
       "10875  10875     NaN      NaN   \n",
       "\n",
       "                                                    text  \n",
       "0      Our Deeds are the Reason of this #earthquake M...  \n",
       "1                 Forest fire near La Ronge Sask. Canada  \n",
       "2      All residents asked to 'shelter in place' are ...  \n",
       "3      13,000 people receive #wildfires evacuation or...  \n",
       "4      Just got sent this photo from Ruby #Alaska as ...  \n",
       "...                                                  ...  \n",
       "10871  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "10872  Storm in RI worse than last hurricane. My city...  \n",
       "10873  Green Line derailment in Chicago http://t.co/U...  \n",
       "10874  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "10875  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[10876 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d31928c2d799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags(data):\n",
    "    pattern = re.compile(r'[@|#][a-zA-Z]+')\n",
    "    matches = pattern.findall(data)\n",
    "    tags = [match[1:].lower() for match in matches]\n",
    "    return ' '.join(tags)\n",
    "\n",
    "def striplinks(data):\n",
    "    cleanr = re.compile(r'(?:(?:http|https|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+')\n",
    "    clean = re.sub(cleanr, ' ', data)\n",
    "    return clean\n",
    "    \n",
    "def alphanumeric(data):\n",
    "    cleanr = re.compile(r'[^A-Za-z0-9]+')\n",
    "    clean = re.sub(cleanr, ' ', data)\n",
    "    return clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer  = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10876/10876 [00:05<00:00, 1931.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run this subsection : 0:00:05.638123\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "with open('processed_total_data.csv', 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['tweets'])\n",
    "    for i in tqdm(range(10876)):\n",
    "        \n",
    "        #raw data\n",
    "        keyword = str(Total_data['keyword'][i])\n",
    "        location = str(Total_data['location'][i])\n",
    "        text = str(Total_data['text'][i])\n",
    "   \n",
    "        #extract tags \n",
    "        tags_str = stemmer.stem(tags(text))\n",
    "        #print(tags_str)\n",
    "        #strip links and alphanumberic charcters\n",
    "        text_sl = striplinks(text)\n",
    "        text_sl_an = alphanumeric(text_sl)\n",
    "        #print(text_sl)\n",
    "        #print(text_sl_an)\n",
    "        #removing stopwords and stem the text       \n",
    "        text_words = word_tokenize(text_sl_an.lower())\n",
    "        text_sl_an_sw_st = ' '.join(stemmer.stem(j) for j in text_words if j not in stop_words )\n",
    "        #print(text_words)\n",
    "        #print(text_sl_an_sw_st)\n",
    "        #assigning weights\n",
    "        processed_list = []\n",
    "        if(tags_str != ''):\n",
    "            processed_list.extend([tags_str,tags_str,tags_str,text_sl_an_sw_st])\n",
    "        else:\n",
    "            processed_list.extend([text_sl_an_sw_st])\n",
    "        if (keyword != 'nan'):\n",
    "            #print(keyword)\n",
    "            processed_list.extend([keyword,keyword])\n",
    "        if (location != 'nan'):\n",
    "            location_words = word_tokenize(alphanumeric(location))\n",
    "            location_str = \" \".join(location_words)\n",
    "            processed_list.extend([location_str])\n",
    "        \n",
    "        #saving string\n",
    "        \n",
    "        processed_string = \" \".join(processed_list)\n",
    "        #print(i, processed_string)\n",
    "        writer.writerow([processed_string])\n",
    "print(\"Time to run this subsection :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_data['keyword'][0] == float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(Total_data['keyword'][0]) == 'nans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'painting is funny thing swim'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"painting is funny thing swimming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"weren't\", 'such', \"she's\", 'these', 'if', 'those', 'and', 'here', 'should', 'its', 'itself', 'i', 'll', 'ours', 'then', 'ma', 'further', 'to', \"haven't\", 't', 'each', 'hadn', 'my', 'while', 'she', 'into', 'him', \"you're\", 'me', 'so', \"hadn't\", \"isn't\", 'd', 'out', 'myself', 'do', 'himself', \"aren't\", 'ourselves', 'when', 'why', 'in', 'too', 'doing', 'until', \"you'd\", \"shouldn't\", 'isn', 'than', 're', 'her', 'not', \"couldn't\", 'are', \"it's\", 'be', 'it', 'just', \"needn't\", 'yourselves', 'had', 'below', 'their', 'up', 'your', \"that'll\", 'aren', 'was', 'm', 'only', 'or', 'that', 'did', 'a', \"wasn't\", \"you've\", 'by', 'between', 'more', 'we', 'being', 'same', 'herself', 'am', 'down', 'don', 'y', 'wouldn', 'no', 's', 'against', 'very', 'any', 'theirs', 'how', 'ain', 'through', 'hasn', \"mightn't\", 'haven', 'all', 'which', 'other', \"you'll\", 'at', 'them', \"hasn't\", 'an', 'does', 'now', 'his', 'some', 'themselves', 'whom', 'once', 'shouldn', 'will', 'most', 'couldn', 'been', 'the', 'but', 'own', 'what', 'nor', 'over', 'above', 'of', \"don't\", 'mightn', 'won', 'both', 'yourself', 'having', 'about', 'our', 'before', 'o', 'shan', \"didn't\", 'has', 'didn', 'on', 'during', 'for', 'from', 'he', 'again', 'were', 'doesn', \"mustn't\", 'needn', 'as', \"won't\", 'can', 'there', 'yours', 've', 'hers', 'with', 'because', \"should've\", 'is', 'weren', 'off', \"doesn't\", 'you', 'have', 'this', \"wouldn't\", 'they', 'after', 'under', 'wasn', 'mustn', \"shan't\", 'who', 'where', 'few'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nokia', ',', 'asd', ';', 'asnkxa', ',', 'INdia']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"nokia, asd;asnkxa , INdia\"\n",
    "word_tokenize(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gloucestershire UK'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphanumeric(\"Gloucestershire,UK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
